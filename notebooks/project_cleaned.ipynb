{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26838c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from tabulate import tabulate\n",
    "import dlib\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "import zipfile\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import filters\n",
    "from tqdm import tqdm\n",
    "import plotly.express as px\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752f3801",
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_path = r\"C:\\Users\\LEGION\\Downloads\\Agedetection\\age_detection.zip\"  # Update this with the correct path\n",
    "\n",
    "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall()\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv('age_detection.csv')\n",
    "\n",
    "# Display the first few rows of the dataframe\n",
    "print(df.head())\n",
    "\n",
    "# Get the file path of the first image\n",
    "image_path = df.loc[0, 'file']\n",
    "\n",
    "# Check if the image path does not contain the absolute path, if not prepend the path\n",
    "if not os.path.isabs(image_path):\n",
    "    image_path = os.path.join(os.path.dirname(zip_path), image_path)\n",
    "\n",
    "# Open and display the image\n",
    "try:\n",
    "    img = Image.open(image_path)\n",
    "    img.show()\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39982ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.columns) \n",
    "print(df.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e215ee5",
   "metadata": {
    "cell_id": "9b485d0ca6364612b99f5e03757cbf09",
    "deepnote_cell_type": "text-cell-h1",
    "formattedRanges": []
   },
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf03c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# File Path Validation\n",
    "df = df[df['file'].apply(lambda x: os.path.isfile(x))]\n",
    "\n",
    "# Duplicate Removal\n",
    "df.drop_duplicates(subset=['file'], keep='first', inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3111f4a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split Consistency\n",
    "df = df[df['split'].isin(['train', 'test'])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd934b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image Quality\n",
    "# removin images that are too small\n",
    "min_width, min_height = 64, 64  # minimum acceptable dimensions\n",
    "def is_image_large_enough(file_path):\n",
    "    with Image.open(file_path) as img:\n",
    "        return img.width >= min_width and img.height >= min_height\n",
    "df = df[df['file'].apply(is_image_large_enough)]\n",
    "\n",
    "# Image Preprocessing\n",
    "# resizing and normalizing images\n",
    "def preprocess_image(file_path):\n",
    "    with Image.open(file_path) as img:\n",
    "        img = img.resize((min_width, min_height))  # resize\n",
    "        img = img.convert('RGB')  # ensure 3 channels\n",
    "        img = np.array(img) / 255.0  # normalize to [0, 1]\n",
    "    return img\n",
    "df['image'] = df['file'].apply(preprocess_image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22e2f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.columns) \n",
    "print(df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1249dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Face Detection\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "\n",
    "def contains_face(file_path):\n",
    "    img = cv2.imread(file_path)\n",
    "    faces = detector(img, 1)\n",
    "    return len(faces) > 0\n",
    "\n",
    "df = df[df['file'].apply(contains_face)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0487bdfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import bz2\n",
    "\n",
    "# URL of the file to be downloaded\n",
    "url = \"http://dlib.net/files/shape_predictor_68_face_landmarks.dat.bz2\"\n",
    "\n",
    "# Path where the downloaded file will be stored\n",
    "output_path = \"shape_predictor_68_face_landmarks.dat.bz2\"\n",
    "\n",
    "# Download the file from `url` and save it locally under `output_path`:\n",
    "urllib.request.urlretrieve(url, output_path)\n",
    "\n",
    "# Open the .bz2 file for reading\n",
    "with bz2.open(output_path, 'rb') as f:\n",
    "    # Decompress the data\n",
    "    decompressed_data = f.read()\n",
    "\n",
    "# Write the decompressed data to a new file\n",
    "with open('shape_predictor_68_face_landmarks.dat', 'wb') as f:\n",
    "    f.write(decompressed_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0049e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Face Alignment\n",
    "predictor = dlib.shape_predictor('shape_predictor_68_face_landmarks.dat')  # Download this file\n",
    "\n",
    "def align_face(file_path):\n",
    "    img = cv2.imread(file_path)\n",
    "    faces = detector(img, 1)\n",
    "    for rect in faces:\n",
    "        shape = predictor(img, rect)\n",
    "        aligned_face = dlib.get_face_chip(img, shape)\n",
    "        return aligned_face\n",
    "\n",
    "df['aligned_face'] = df['file'].apply(align_face)\n",
    "\n",
    "# Background Removal\n",
    "\n",
    "# Lighting and Color Normalization\n",
    "def normalize_image(face):\n",
    "    gray = cv2.cvtColor(face, cv2.COLOR_BGR2GRAY)\n",
    "    normalized = cv2.equalizeHist(gray)\n",
    "    return normalized\n",
    "\n",
    "df['normalized_face'] = df['aligned_face'].apply(normalize_image)\n",
    "\n",
    "#Data Augmentation\n",
    "def augment_image(face):\n",
    "    M = cv2.getRotationMatrix2D((face.shape[1] / 2, face.shape[0] / 2), np.random.uniform(-30, 30), 1)\n",
    "    rotated = cv2.warpAffine(face, M, (face.shape[1], face.shape[0]))\n",
    "    return rotated\n",
    "\n",
    "df['augmented_face'] = df['normalized_face'].apply(augment_image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d929d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Eencoding age\n",
    "label_encoder = LabelEncoder()\n",
    "df['ageLabel'] = label_encoder.fit_transform(df['age'])\n",
    "df['ageLabel']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b972883f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_images(images, labels, num_images=5):\n",
    "    fig, axes = plt.subplots(1, num_images, figsize=(15, 3))\n",
    "    \n",
    "    for i in range(num_images):\n",
    "        axes[i].imshow(images[i])\n",
    "        axes[i].set_title(f'Label: {labels[i]}')\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "# Display the first 5 images from the training set\n",
    "display_images(df['augmented_face'][:5], label_encoder.inverse_transform(df['ageLabel'])[:5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a91880",
   "metadata": {
    "cell_id": "1d801dd8159f4a92a8d0fbe56cec697a",
    "deepnote_cell_type": "text-cell-h1",
    "formattedRanges": []
   },
   "source": [
    "# EDA and feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f73a614",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting the age and their disturbution\n",
    "\n",
    "\n",
    "age_counts = df['label'].value_counts()\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(12, 6))\n",
    "age_counts.plot(kind='bar', color='skyblue')\n",
    "plt.title('Age Distribution')\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec38aad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "age_counts = df['age'].value_counts()\n",
    "age_counts.index\n",
    "age_counts.values\n",
    "age_counts_df = pd.DataFrame({'age':age_counts.index,'Counts':age_counts.values})\n",
    "px.bar(data_frame=age_counts_df,\n",
    " x='age',\n",
    " y='Counts',\n",
    " color='Counts',\n",
    " color_continuous_scale='blues',\n",
    " text_auto=True,\n",
    " title=f'Age Distribution')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e023b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#showing images\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Function to display images\n",
    "def display_images(images, labels, num_images=5):\n",
    "    fig, axes = plt.subplots(1, num_images, figsize=(15, 3))\n",
    "    \n",
    "    for i in range(num_images):\n",
    "        axes[i].imshow(images[i])\n",
    "        axes[i].set_title(f'Label: {labels[i]}')\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "# Display the first 5 images from the training set\n",
    "display_images(df['augmented_face'][:5], label_encoder.inverse_transform(df['ageLabel'])[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f65455",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate mean and standard deviation along each channel\n",
    "\n",
    "image_array = np.array(train_images)\n",
    "\n",
    "mean_values = np.mean(image_array, axis=(0, 1, 2))\n",
    "std_values = np.std(image_array, axis=(0, 1, 2))\n",
    "\n",
    "# Create a DataFrame for better presentation\n",
    "statistics_df = pd.DataFrame({\n",
    "    'Channel': ['Red', 'Green', 'Blue'],\n",
    "    'Mean': mean_values,\n",
    "    'Standard Deviation': std_values\n",
    "})\n",
    "\n",
    "# Display the DataFrame with a styled format\n",
    "styled_df = statistics_df.style.format({\n",
    "    'Mean': '{:.2f}',\n",
    "    'Standard Deviation': '{:.2f}'\n",
    "})\n",
    "styled_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d1b78d",
   "metadata": {
    "cell_id": "8dc363825a2a45f2ac0872da4ddc1f82",
    "deepnote_cell_type": "text-cell-h3",
    "formattedRanges": []
   },
   "source": [
    "### skin texture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26171cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column in the DataFrame to store texture features\n",
    "df['texture_features'] = None\n",
    "\n",
    "# Function to extract basic texture features from an image\n",
    "def get_texture_features(image_path):\n",
    "    # Load the image\n",
    "    image = cv2.imread(image_path)\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Apply Gaussian blur to smooth the image\n",
    "    blurred = cv2.GaussianBlur(gray, (5, 5), 0)\n",
    "\n",
    "    # Compute texture features using the Laplacian operator\n",
    "    laplacian = cv2.Laplacian(blurred, cv2.CV_64F)\n",
    "    laplacian_var = np.var(laplacian)\n",
    "\n",
    "    return laplacian_var\n",
    "\n",
    "# Apply the get_texture_features function to each image in the DataFrame\n",
    "texture_features_list = []\n",
    "for image_path in tqdm(train_df['filepaths']):\n",
    "    texture_features = get_texture_features(image_path)\n",
    "    texture_features_list.append(texture_features)\n",
    "\n",
    "# Add the extracted texture features to the DataFrame\n",
    "df['texture_features'] = texture_features_list\n",
    "\n",
    "# Visualize Texture Features Across Age Groups\n",
    "age_groups = df.groupby('label')\n",
    "\n",
    "# Set up subplots for visualization\n",
    "fig, axs = plt.subplots(len(age_groups), figsize=(8, 5 * len(age_groups)))\n",
    "\n",
    "for i, (age_group, group_data) in enumerate(age_groups):\n",
    "    axs[i].set_title(f\"Age Group: {age_group}\")\n",
    "\n",
    "    # Visualize texture features for a sample of images in the age group\n",
    "    axs[i].hist(group_data['texture_features'], bins=20, color='skyblue', edgecolor='black')\n",
    "    axs[i].set_xlabel('Texture Features')\n",
    "    axs[i].set_ylabel('Frequency')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59719dc3",
   "metadata": {
    "cell_id": "fa26cac6d389439890d43ef06327238b",
    "deepnote_cell_type": "text-cell-h3",
    "formattedRanges": []
   },
   "source": [
    "### wrinkle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a7fd724",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column in the DataFrame to store wrinkle features\n",
    "df['wrinkle_features'] = None\n",
    "\n",
    "# Function to extract basic wrinkle features from an image\n",
    "def get_wrinkle_features(image_path):\n",
    "    # Load the image\n",
    "    image = cv2.imread(image_path)\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Apply Canny edge detection to enhance wrinkles\n",
    "    edges = cv2.Canny(gray, 50, 150)\n",
    "\n",
    "    # Compute the percentage of white pixels in the edges\n",
    "    wrinkle_percentage = np.sum(edges) / (gray.shape[0] * gray.shape[1])\n",
    "\n",
    "    return wrinkle_percentage\n",
    "\n",
    "# Apply the get_wrinkle_features function to each image in the DataFrame\n",
    "wrinkle_features_list = []\n",
    "for image_path in tqdm(train_df['filepaths']):\n",
    "    wrinkle_features = get_wrinkle_features(image_path)\n",
    "    wrinkle_features_list.append(wrinkle_features)\n",
    "\n",
    "# Add the extracted wrinkle features to the DataFrame\n",
    "df['wrinkle_features'] = wrinkle_features_list\n",
    "\n",
    "# Visualize Wrinkle Features Across Age Groups\n",
    "age_groups = df.groupby('label')\n",
    "\n",
    "# Set up subplots for visualization\n",
    "fig, axs = plt.subplots(len(age_groups), figsize=(8, 5 * len(age_groups)))\n",
    "\n",
    "for i, (age_group, group_data) in enumerate(age_groups):\n",
    "    axs[i].set_title(f\"Age Group: {age_group}\")\n",
    "\n",
    "    # Visualize wrinkle features for a sample of images in the age group\n",
    "    axs[i].hist(group_data['wrinkle_features'], bins=20, color='lightcoral', edgecolor='black')\n",
    "    axs[i].set_xlabel('Wrinkle Features')\n",
    "    axs[i].set_ylabel('Frequency')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74ab75b",
   "metadata": {
    "cell_id": "23687019cba64ef3b71233c96780a413",
    "deepnote_cell_type": "text-cell-h3",
    "formattedRanges": []
   },
   "source": [
    "### hair color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4617c5bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#probably useless\n",
    "\n",
    "def extract_hair_color(image_path):\n",
    "    # Load the image\n",
    "    image = cv2.imread(image_path)\n",
    "    # Convert the image to HSV color space for better color analysis\n",
    "    hsv_image = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "    \n",
    "    # Define a mask for the hair region in the HSV color space\n",
    "    lower_hair_color = np.array([0, 10, 40])\n",
    "    upper_hair_color = np.array([30, 200, 255])\n",
    "    hair_mask = cv2.inRange(hsv_image, lower_hair_color, upper_hair_color)\n",
    "    \n",
    "    # Apply the hair mask to the original image\n",
    "    hair_region = cv2.bitwise_and(image, image, mask=hair_mask)\n",
    "    \n",
    "    # Calculate the dominant hair color\n",
    "    dominant_color = np.mean(hair_region, axis=(0, 1)).astype(int)\n",
    "    \n",
    "    return dominant_color\n",
    "\n",
    "# Apply the extract_hair_color function to each image in the DataFrame\n",
    "df['hair_color'] = df['filepaths'].apply(extract_hair_color)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e593ec6",
   "metadata": {
    "cell_id": "0f6f85f193f2439b9e9d814fc1a31dd9",
    "deepnote_cell_type": "text-cell-h2",
    "formattedRanges": []
   },
   "source": [
    "## facial landmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f643ee54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import dlib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the pre-trained facial landmark predictor\n",
    "predictor_path = \"dlib/shape_predictor_68_face_landmarks.dat\"  # Replace with the path to the shape predictor model\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor(predictor_path)\n",
    "\n",
    "# Function to detect facial landmarks in an image\n",
    "def detect_landmarks(image_pixels):\n",
    "    # Convert the list of pixel values to a NumPy array\n",
    "    image_array = np.array(image_pixels, dtype=np.uint8)\n",
    "\n",
    "    # Detect faces in the image\n",
    "    faces = detector(image_array)\n",
    "\n",
    "    # Loop over each face and get facial landmarks\n",
    "    landmarks_list = []\n",
    "    for face in faces:\n",
    "        shape = predictor(image_array, face)\n",
    "        landmarks = [(shape.part(i).x, shape.part(i).y) for i in range(shape.num_parts)]\n",
    "        landmarks_list.append(landmarks)\n",
    "\n",
    "        # Draw landmarks on the image (comment out if you don't want to display)\n",
    "        for (x, y) in landmarks:\n",
    "            cv2.circle(image_array, (x, y), 2, (0, 255, 0), -1)\n",
    "\n",
    "    # Display the image with landmarks (comment out if you don't want to display)\n",
    "    cv2.imshow(\"Facial Landmarks\", image_array)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "    return landmarks_list\n",
    "\n",
    "# Apply the detect_landmarks function to each image in the DataFrame\n",
    "df['landmarks'] = df['augmented_face'].apply(detect_landmarks)\n",
    "\n",
    "# Now df['landmarks'] contains the detected facial landmarks for each image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfed776c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#eye openness detection (im not sure how accurate this is and if it works)\n",
    "\n",
    "import cv2\n",
    "import dlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Load the pre-trained facial landmark predictor\n",
    "predictor_path = \"dlib/shape_predictor_68_face_landmarks.dat\"  # Replace with the path to the shape predictor model\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor(predictor_path)\n",
    "\n",
    "# Function to detect facial landmarks and extract eye-related features\n",
    "def detect_eye_features(image_pixels):\n",
    "    # Convert the list of pixel values to a NumPy array\n",
    "    image_array = np.array(image_pixels, dtype=np.uint8)\n",
    "\n",
    "    # Ensure the image has 3 channels (for compatibility with cv2.COLOR_BGR2GRAY)\n",
    "    if image_array.ndim == 2:\n",
    "        image_array = cv2.cvtColor(image_array, cv2.COLOR_GRAY2BGR)\n",
    "\n",
    "    # Convert the image to grayscale\n",
    "    gray = cv2.cvtColor(image_array, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Detect faces in the image\n",
    "    faces = detector(gray)\n",
    "\n",
    "    # Check if a face is detected\n",
    "    if len(faces) > 0:\n",
    "        # Get facial landmarks for the first detected face\n",
    "        shape = predictor(gray, faces[0])\n",
    "\n",
    "        # Extract eye-related features\n",
    "        left_eye_openness = shape.part(47).y - shape.part(43).y  # Example: vertical distance between eyebrow and lower eyelid\n",
    "        right_eye_openness = shape.part(40).y - shape.part(38).y\n",
    "\n",
    "       \n",
    "\n",
    "    return left_eye_openness, right_eye_openness\n",
    "\n",
    "    return None\n",
    "\n",
    "# Example: Apply the detect_eye_features function to each image in the DataFrame\n",
    "eye_features = df['normalized_face'].apply(detect_eye_features)\n",
    "\n",
    "# Example: Add the extracted eye features to the DataFrame\n",
    "df[['left_eye_openness', 'right_eye_openness']] = pd.DataFrame(eye_features.tolist(), index=df.index)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4092800a",
   "metadata": {
    "cell_id": "02445ac522364f00b361023dd5916fe1",
    "deepnote_cell_type": "text-cell-h3",
    "formattedRanges": []
   },
   "source": [
    "### cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23639e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import f1_score, mean_squared_error, mean_absolute_error\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, Flatten, Input, Concatenate\n",
    "\n",
    "\n",
    "# Constants for image preprocessing\n",
    "desired_width = 128\n",
    "desired_height = 128\n",
    "num_channels = 3  # Assuming color images, adjust if grayscale\n",
    "\n",
    "\n",
    "# Step 1: Prepare Data\n",
    "# Extract 'eye openness', 'texture features', and 'wrinkle features'\n",
    "tabular_features = df[['texture_features', 'wrinkle_features', 'left_eye_openness','right_eye_openness']]\n",
    "# Standardize numerical features\n",
    "scaler = StandardScaler()\n",
    "tabular_features = scaler.fit_transform(tabular_features)\n",
    "\n",
    "\n",
    "desired_channels = 3 \n",
    "\n",
    "def preprocess_image(image):\n",
    "    # Resize to a common size\n",
    "    resized_image = cv2.resize(image, (desired_width, desired_height))\n",
    "    # Ensure the image has the correct number of channels\n",
    "    if resized_image.shape[-1] != desired_channels:\n",
    "        # If the image has more than 3 channels, convert to grayscale\n",
    "        resized_image = cv2.cvtColor(resized_image, cv2.COLOR_BGR2GRAY)\n",
    "        # Add an extra dimension to represent the single channel\n",
    "        resized_image = np.expand_dims(resized_image, axis=-1)\n",
    "    # Normalize pixel values to be between 0 and 1\n",
    "    resized_image = resized_image / 255.0\n",
    "    # Add other preprocessing steps as needed (e.g., data augmentation)\n",
    "    return resized_image\n",
    "\n",
    "# Load and preprocess images\n",
    "X_images = np.array([preprocess_image(image) for image in df['image'].values])\n",
    "# Extract target\n",
    "y_target = df['ageLabel'].values\n",
    "\n",
    "# Split data into training and testing sets based on 'split' column\n",
    "train_indices = df[df['split'] == 'train'].index\n",
    "test_indices = df[df['split'] == 'test'].index\n",
    "\n",
    "X_images_train = X_images[train_indices]\n",
    "X_tabular_train = tabular_features[train_indices]\n",
    "y_train = y_target[train_indices]\n",
    "\n",
    "X_images_test = X_images[test_indices]\n",
    "X_tabular_test = tabular_features[test_indices]\n",
    "y_test = y_target[test_indices]\n",
    "\n",
    "# The rest of your code remains the same...\n",
    "\n",
    "# Step 2: Build Models\n",
    "# Build CNN model for image feature extraction\n",
    "image_input = Input(shape=(desired_width, desired_height, num_channels))\n",
    "x = Conv2D(32, (3, 3), activation='relu')(image_input)\n",
    "x = MaxPooling2D((2, 2))(x)\n",
    "x = Conv2D(64, (3, 3), activation='relu')(x)\n",
    "x = MaxPooling2D((2, 2))(x)\n",
    "x = Flatten()(x)\n",
    "image_output = Dense(128, activation='relu')(x)\n",
    "\n",
    "# Build model for tabular features\n",
    "tabular_input = Input(shape=(X_tabular_train.shape[1],))\n",
    "tabular_output = Dense(128, activation='relu')(tabular_input)\n",
    "\n",
    "# Step 3: Combine Models\n",
    "concatenated = Concatenate(axis=-1)([image_output, tabular_output])\n",
    "x = Dense(64, activation='relu')(concatenated)\n",
    "output = Dense(1, activation='linear')(x)\n",
    "\n",
    "combined_model = Model(inputs=[image_input, tabular_input], outputs=output)\n",
    "\n",
    "# Step 4: Train and Evaluate\n",
    "combined_model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "combined_model.fit([X_images_train, X_tabular_train], y_train, epochs=10, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = combined_model.predict([X_images_test, X_tabular_test])\n",
    "\n",
    "# Calculate metrics\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "# Assuming you have binary classification labels for f1_score\n",
    "y_test_binary = (y_test > 0).astype(int)\n",
    "y_pred_binary = (y_pred > 0).astype(int)\n",
    "f1 = f1_score(y_test_binary, y_pred_binary)\n",
    "\n",
    "print(f'Mean Absolute Error: {mae}')\n",
    "print(f'Mean Squared Error: {mse}')\n",
    "print(f'F1 Score: {f1}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100a6833",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import f1_score, mean_squared_error, mean_absolute_error\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Conv2D, MaxPooling2D, Flatten, Input, Concatenate\n",
    "\n",
    "# Assuming 'df' is your DataFrame\n",
    "\n",
    "# Constants for image preprocessing\n",
    "desired_width = 128\n",
    "desired_height = 128\n",
    "num_channels = 3  # Assuming color images, adjust if grayscale\n",
    "\n",
    "# Step 1: Prepare Data\n",
    "# Extract 'eye openness', 'texture features', and 'wrinkle features'\n",
    "tabular_features = df_cleaned[['texture_features', 'wrinkle_features', 'left_eye_openness','right_eye_openness']]\n",
    "# Standardize numerical features\n",
    "scaler = StandardScaler()\n",
    "tabular_features = scaler.fit_transform(tabular_features)\n",
    "\n",
    "# Load and preprocess images\n",
    "desired_channels = 3  # Assuming color images, adjust if grayscale\n",
    "\n",
    "def preprocess_image(image):\n",
    "    # Resize to a common size\n",
    "    resized_image = cv2.resize(image, (desired_width, desired_height))\n",
    "    # Ensure the image has the correct number of channels\n",
    "    if resized_image.shape[-1] != desired_channels:\n",
    "        # If the image has more than 3 channels, convert to grayscale\n",
    "        resized_image = cv2.cvtColor(resized_image, cv2.COLOR_BGR2GRAY)\n",
    "        # Add an extra dimension to represent the single channel\n",
    "        resized_image = np.expand_dims(resized_image, axis=-1)\n",
    "    # Normalize pixel values to be between 0 and 1\n",
    "    resized_image = resized_image / 255.0\n",
    "    # Add other preprocessing steps as needed (e.g., data augmentation)\n",
    "    return resized_image\n",
    "\n",
    "X_images = np.array([preprocess_image(image) for image in df_cleaned['image'].values])\n",
    "# Extract target\n",
    "y_target = df_cleaned['ageLabel'].values\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_images_train, X_images_test, X_tabular_train, X_tabular_test, y_train, y_test = train_test_split(\n",
    "    X_images, tabular_features, y_target, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Step 2: Build Models\n",
    "# Build CNN model for image feature extraction\n",
    "image_input = Input(shape=(desired_width, desired_height, num_channels))\n",
    "x = Conv2D(32, (3, 3), activation='relu')(image_input)\n",
    "x = MaxPooling2D((2, 2))(x)\n",
    "x = Conv2D(64, (3, 3), activation='relu')(x)\n",
    "x = MaxPooling2D((2, 2))(x)\n",
    "x = Flatten()(x)\n",
    "image_output = Dense(128, activation='relu')(x)\n",
    "\n",
    "# Build model for tabular features\n",
    "tabular_input = Input(shape=(X_tabular_train.shape[1],))\n",
    "tabular_output = Dense(128, activation='relu')(tabular_input)\n",
    "\n",
    "# Step 3: Combine Models\n",
    "concatenated = Concatenate(axis=-1)([image_output, tabular_output])\n",
    "x = Dense(64, activation='relu')(concatenated)\n",
    "output = Dense(1, activation='linear')(x)\n",
    "\n",
    "combined_model = Model(inputs=[image_input, tabular_input], outputs=output)\n",
    "\n",
    "# Step 4: Train and Evaluate\n",
    "combined_model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "combined_model.fit([X_images_train, X_tabular_train], y_train, epochs=10, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = combined_model.predict([X_images_test, X_tabular_test])\n",
    "\n",
    "# Calculate metrics\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "y_test_binary = (y_test > 0).astype(int)\n",
    "y_pred_binary = (y_pred > 0).astype(int)\n",
    "f1 = f1_score(y_test_binary, y_pred_binary)\n",
    "\n",
    "print(f'Mean Absolute Error: {mae}')\n",
    "print(f'Mean Squared Error: {mse}')\n",
    "print(f'F1 Score: {f1}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a78e3579",
   "metadata": {
    "cell_id": "eab26478c4594aeaa973cf35326a4c9a",
    "deepnote_cell_type": "text-cell-p",
    "formattedRanges": []
   },
   "source": [
    "Mean Absolute Error: 1.2200337251027424\n",
    "Mean Squared Error: 2.036018026809009\n",
    "F1 Score: 0.8679245283018869"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2104d105",
   "metadata": {
    "cell_id": "941c346dffc9424483a989a742250c3d",
    "deepnote_cell_type": "text-cell-h3",
    "formattedRanges": []
   },
   "source": [
    "### vggface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "510d1089",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import f1_score, mean_squared_error, mean_absolute_error\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Conv2D, MaxPooling2D, Flatten, Input, Concatenate\n",
    "from keras_vggface.vggface import VGGFace\n",
    "from keras_vggface.utils import preprocess_input\n",
    "\n",
    "# Assuming 'df_cleaned' is your DataFrame\n",
    "\n",
    "# Constants for image preprocessing\n",
    "desired_width = 128\n",
    "desired_height = 128\n",
    "num_channels = 3  # Assuming color images, adjust if grayscale\n",
    "\n",
    "# Step 1: Prepare Data\n",
    "# Extract 'eye openness', 'texture features', and 'wrinkle features'\n",
    "tabular_features = df_cleaned[['texture_features', 'wrinkle_features', 'left_eye_openness','right_eye_openness']]\n",
    "# Standardize numerical features\n",
    "scaler = StandardScaler()\n",
    "tabular_features = scaler.fit_transform(tabular_features)\n",
    "\n",
    "# Load and preprocess images\n",
    "desired_channels = 3  # Assuming color images, adjust if grayscale\n",
    "\n",
    "def preprocess_image(image):\n",
    "    # Resize to a common size\n",
    "    resized_image = cv2.resize(image, (desired_width, desired_height))\n",
    "    # Ensure the image has the correct number of channels\n",
    "    if resized_image.shape[-1] != desired_channels:\n",
    "        # If the image has more than 3 channels, convert to grayscale\n",
    "        resized_image = cv2.cvtColor(resized_image, cv2.COLOR_BGR2GRAY)\n",
    "        # Add an extra dimension to represent the single channel\n",
    "        resized_image = np.expand_dims(resized_image, axis=-1)\n",
    "    # Normalize pixel values to be between 0 and 1\n",
    "    resized_image = resized_image / 255.0\n",
    "    # Add other preprocessing steps as needed (e.g., data augmentation)\n",
    "    return resized_image\n",
    "\n",
    "X_images = np.array([preprocess_image(image) for image in df_cleaned['image'].values])\n",
    "# Extract target\n",
    "y_target = df_cleaned['ageLabel'].values\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_images_train, X_images_test, X_tabular_train, X_tabular_test, y_train, y_test = train_test_split(\n",
    "    X_images, tabular_features, y_target, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Step 2: Build Models\n",
    "# Build VGGFace model for image feature extraction\n",
    "image_input = Input(shape=(desired_width, desired_height, num_channels))\n",
    "vgg_model = VGGFace(model='vgg16', include_top=False, input_tensor=image_input)\n",
    "\n",
    "# Freeze layers to retain pretrained weights\n",
    "for layer in vgg_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "x = Flatten()(vgg_model.output)\n",
    "image_output = Dense(128, activation='relu')(x)\n",
    "\n",
    "# Build model for tabular features\n",
    "tabular_input = Input(shape=(X_tabular_train.shape[1],))\n",
    "tabular_output = Dense(128, activation='relu')(tabular_input)\n",
    "\n",
    "# Step 3: Combine Models\n",
    "concatenated = Concatenate(axis=-1)([image_output, tabular_output])\n",
    "x = Dense(64, activation='relu')(concatenated)\n",
    "output = Dense(1, activation='linear')(x)\n",
    "\n",
    "combined_model = Model(inputs=[image_input, tabular_input], outputs=output)\n",
    "\n",
    "# Step 4: Train and Evaluate\n",
    "combined_model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "combined_model.fit([X_images_train, X_tabular_train], y_train, epochs=10, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = combined_model.predict([X_images_test, X_tabular_test])\n",
    "\n",
    "# Calculate metrics\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "y_test_binary = (y_test > 0).astype(int)\n",
    "y_pred_binary = (y_pred > 0).astype(int)\n",
    "f1 = f1_score(y_test_binary, y_pred_binary)\n",
    "\n",
    "print(f'Mean Absolute Error: {mae}')\n",
    "print(f'Mean Squared Error: {mse}')\n",
    "print(f'F1 Score: {f1}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df897f9e",
   "metadata": {
    "cell_id": "5b9ec9a2acf8452dbdccc405f625379e",
    "deepnote_cell_type": "text-cell-p",
    "formattedRanges": []
   },
   "source": [
    "Mean Absolute Error: 1.288718291123708\n",
    "Mean Squared Error: 2.2314737071360065\n",
    "F1 Score: 0.8679245283018869"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "199142d3",
   "metadata": {
    "cell_id": "48335019686d4e4ebd7fd57c22d1337c",
    "deepnote_cell_type": "text-cell-h1",
    "formattedRanges": []
   },
   "source": [
    "# streamlit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e704178",
   "metadata": {},
   "outputs": [],
   "source": [
    "#building up the model\n",
    "\n",
    "import os\n",
    "import zipfile\n",
    "import cv2\n",
    "import dlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import f1_score, mean_squared_error, mean_absolute_error\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Conv2D, MaxPooling2D, Flatten, Input, Concatenate\n",
    "\n",
    "def load_and_preprocess_data():\n",
    "    # Load your data \n",
    "    zip_path = \"dataset/age.zip\"  \n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall()\n",
    "    df = pd.read_csv('age_detection.csv')\n",
    "    image_path = df.loc[0, 'file']\n",
    "    if not os.path.isabs(image_path):\n",
    "        image_path = os.path.join(os.path.dirname(zip_path), image_path)\n",
    "    df = df[df['file'].apply(lambda x: os.path.isfile(x))]\n",
    "    # Duplicate Removal\n",
    "    df.drop_duplicates(subset=['file'], keep='first', inplace=True)\n",
    "    df = df[df['split'].isin(['train', 'test'])]    \n",
    "    # Image Quality\n",
    "    # removing images that are too small\n",
    "    min_width, min_height = 64, 64  # minimum acceptable dimensions\n",
    "    def is_image_large_enough(file_path):\n",
    "        with Image.open(file_path) as img:\n",
    "            return img.width >= min_width and img.height >= min_height\n",
    "    df = df[df['file'].apply(is_image_large_enough)]\n",
    "\n",
    "    # Image Preprocessing\n",
    "    # resizing and normalizing images\n",
    "    def preprocess_image(file_path):\n",
    "        with Image.open(file_path) as img:\n",
    "            img = img.resize((min_width, min_height))  # resize\n",
    "            img = img.convert('RGB')  # ensure 3 channels\n",
    "            img = np.array(img) / 255.0  # normalize to [0, 1]\n",
    "        return img\n",
    "    df['image'] = df['file'].apply(preprocess_image)\n",
    "    detector = dlib.get_frontal_face_detector()\n",
    "\n",
    "    def contains_face(file_path):\n",
    "        img = cv2.imread(file_path)\n",
    "        faces = detector(img, 1)\n",
    "        return len(faces) > 0\n",
    "\n",
    "    df = df[df['file'].apply(contains_face)]\n",
    "    predictor = dlib.shape_predictor('dlib/shape_predictor_68_face_landmarks.dat')  # Download this file\n",
    "\n",
    "    def align_face(file_path):\n",
    "        img = cv2.imread(file_path)\n",
    "        faces = detector(img, 1)\n",
    "        for rect in faces:\n",
    "            shape = predictor(img, rect)\n",
    "            aligned_face = dlib.get_face_chip(img, shape)\n",
    "            return aligned_face\n",
    "\n",
    "    df['aligned_face'] = df['file'].apply(align_face)\n",
    "\n",
    "    # Background Removal\n",
    "\n",
    "    # Lighting and Color Normalization\n",
    "    def normalize_image(face):\n",
    "        gray = cv2.cvtColor(face, cv2.COLOR_BGR2GRAY)\n",
    "        normalized = cv2.equalizeHist(gray)\n",
    "        return normalized\n",
    "\n",
    "    df['normalized_face'] = df['aligned_face'].apply(normalize_image)\n",
    "\n",
    "    # Data Augmentation\n",
    "    def augment_image(face):\n",
    "        M = cv2.getRotationMatrix2D((face.shape[1] / 2, face.shape[0] / 2), np.random.uniform(-30, 30), 1)\n",
    "        rotated = cv2.warpAffine(face, M, (face.shape[1], face.shape[0]))\n",
    "        return rotated\n",
    "\n",
    "    df['augmented_face'] = df['normalized_face'].apply(augment_image)\n",
    "    label_encoder = LabelEncoder()\n",
    "    df['ageLabel'] = label_encoder.fit_transform(df['age'])\n",
    "\n",
    "    # Create a new column in the DataFrame to store texture features\n",
    "    df['texture_features'] = None\n",
    "\n",
    "    # Function to extract basic texture features from an image\n",
    "    def get_texture_features(image_path):\n",
    "        # Load the image\n",
    "        image = cv2.imread(image_path)\n",
    "        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # Apply Gaussian blur to smooth the image\n",
    "        blurred = cv2.GaussianBlur(gray, (5, 5), 0)\n",
    "\n",
    "        # Compute texture features using the Laplacian operator\n",
    "        laplacian = cv2.Laplacian(blurred, cv2.CV_64F)\n",
    "        laplacian_var = np.var(laplacian)\n",
    "\n",
    "        return laplacian_var\n",
    "\n",
    "    # Apply the get_texture_features function to each image in the DataFrame\n",
    "    texture_features_list = []\n",
    "    for image_path in tqdm(df['file']):\n",
    "        texture_features = get_texture_features(image_path)\n",
    "        texture_features_list.append(texture_features)\n",
    "\n",
    "    # Add the extracted texture features to the DataFrame\n",
    "    df['texture_features'] = texture_features_list\n",
    "\n",
    "    df['wrinkle_features'] = None\n",
    "\n",
    "    # Function to extract basic wrinkle features from an image\n",
    "    def get_wrinkle_features(image_path):\n",
    "        # Load the image\n",
    "        image = cv2.imread(image_path)\n",
    "        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # Apply Canny edge detection to enhance wrinkles\n",
    "        edges = cv2.Canny(gray, 50, 150)\n",
    "\n",
    "        # Compute the percentage of white pixels in the edges\n",
    "        wrinkle_percentage = np.sum(edges) / (gray.shape[0] * gray.shape[1])\n",
    "\n",
    "        return wrinkle_percentage\n",
    "\n",
    "    # Apply the get_wrinkle_features function to each image in the DataFrame\n",
    "    wrinkle_features_list = []\n",
    "    for image_path in tqdm(df['file']):\n",
    "        wrinkle_features = get_wrinkle_features(image_path)\n",
    "        wrinkle_features_list.append(wrinkle_features)\n",
    "\n",
    "    # Add the extracted wrinkle features to the DataFrame\n",
    "    df['wrinkle_features'] = wrinkle_features_list\n",
    "\n",
    "    # Load the pre-trained facial landmark predictor\n",
    "    predictor_path = \"dlib/shape_predictor_68_face_landmarks.dat\"  # Replace with the path to the shape predictor model\n",
    "    detector = dlib.get_frontal_face_detector()\n",
    "    predictor = dlib.shape_predictor(predictor_path)\n",
    "\n",
    "    # Function to detect facial landmarks and extract eye-related features\n",
    "    def detect_eye_features(image_pixels):\n",
    "        # Convert the list of pixel values to a NumPy array\n",
    "        image_array = np.array(image_pixels, dtype=np.uint8)\n",
    "\n",
    "        # Ensure the image has 3 channels (for compatibility with cv2.COLOR_BGR2GRAY)\n",
    "        if image_array.ndim == 2:\n",
    "            image_array = cv2.cvtColor(image_array, cv2.COLOR_GRAY2BGR)\n",
    "\n",
    "        # Convert the image to grayscale\n",
    "        gray = cv2.cvtColor(image_array, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # Detect faces in the image\n",
    "        faces = detector(gray)\n",
    "\n",
    "        # Check if a face is detected\n",
    "        if len(faces) > 0:\n",
    "            # Get facial landmarks for the first detected face\n",
    "            shape = predictor(gray, faces[0])\n",
    "\n",
    "            # Extract eye-related features\n",
    "            left_eye_openness = shape.part(47).y - shape.part(43).y  # Example: vertical distance between eyebrow and lower eyelid\n",
    "            right_eye_openness = shape.part(40).y - shape.part(38).y\n",
    "\n",
    "            return left_eye_openness, right_eye_openness\n",
    "\n",
    "    # Example: Apply the detect_eye_features function to each image in the DataFrame\n",
    "    eye_features = df['normalized_face'].apply(detect_eye_features)\n",
    "\n",
    "    # Example: Add the extracted eye features to the DataFrame\n",
    "    df[['left_eye_openness', 'right_eye_openness']] = pd.DataFrame(eye_features.tolist(), index=df.index)\n",
    "    df = df.dropna()\n",
    "\n",
    "    return df\n",
    "\n",
    "def load_model(df_cleaned):\n",
    "    # Load the trained model\n",
    "    desired_width = 128\n",
    "    desired_height = 128\n",
    "    num_channels = 3  # Assuming color images, adjust if grayscale\n",
    "\n",
    "    # Step 1: Prepare Data\n",
    "    # Extract 'eye openness', 'texture features', and 'wrinkle features'\n",
    "    tabular_features = df_cleaned[['texture_features', 'wrinkle_features', 'left_eye_openness','right_eye_openness']]\n",
    "    # Standardize numerical features\n",
    "    scaler = StandardScaler()\n",
    "    tabular_features = scaler.fit_transform(tabular_features)\n",
    "\n",
    "    # Load and preprocess images\n",
    "    desired_channels = 3  # Assuming color images, adjust if grayscale\n",
    "\n",
    "    def preprocess_image(image):\n",
    "        # Resize to a common size\n",
    "        resized_image = cv2.resize(image, (desired_width, desired_height))\n",
    "        # Ensure the image has the correct number of channels\n",
    "        if resized_image.shape[-1] != desired_channels:\n",
    "            # If the image has more than 3 channels, convert to grayscale\n",
    "            resized_image = cv2.cvtColor(resized_image, cv2.COLOR_BGR2GRAY)\n",
    "            # Add an extra dimension to represent the single channel\n",
    "            resized_image = np.expand_dims(resized_image, axis=-1)\n",
    "        # Normalize pixel values to be between 0 and 1\n",
    "        resized_image = resized_image / 255.0\n",
    "        # Add other preprocessing steps as needed (e.g., data augmentation)\n",
    "        return resized_image\n",
    "\n",
    "    X_images = np.array([preprocess_image(image) for image in df_cleaned['image'].values])\n",
    "    # Extract target\n",
    "    y_target = df_cleaned['ageLabel'].values\n",
    "\n",
    "    # Split data into training and testing sets\n",
    "    X_images_train, X_images_test, X_tabular_train, X_tabular_test, y_train, y_test = train_test_split(\n",
    "        X_images, tabular_features, y_target, test_size=0.2, random_state=42\n",
    "    )\n",
    "\n",
    "    # Step 2: Build Models\n",
    "    # Build CNN model for image feature extraction\n",
    "    image_input = Input(shape=(desired_width, desired_height, num_channels))\n",
    "    x = Conv2D(32, (3, 3), activation='relu')(image_input)\n",
    "    x = MaxPooling2D((2, 2))(x)\n",
    "    x = Conv2D(64, (3, 3), activation='relu')(x)\n",
    "    x = MaxPooling2D((2, 2))(x)\n",
    "    x = Flatten()(x)\n",
    "    image_output = Dense(128, activation='relu')(x)\n",
    "\n",
    "    # Build model for tabular features\n",
    "    tabular_input = Input(shape=(X_tabular_train.shape[1],))\n",
    "    tabular_output = Dense(128, activation='relu')(tabular_input)\n",
    "\n",
    "    # Step 3: Combine Models\n",
    "    concatenated = Concatenate(axis=-1)([image_output, tabular_output])\n",
    "    x = Dense(64, activation='relu')(concatenated)\n",
    "    output = Dense(1, activation='linear')(x)\n",
    "\n",
    "    combined_model = Model(inputs=[image_input, tabular_input], outputs=output)\n",
    "\n",
    "    # Step 4: Train and Evaluate\n",
    "    combined_model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    combined_model.fit([X_images_train, X_tabular_train], y_train, epochs=10, batch_size=32, validation_split=0.2)\n",
    "\n",
    "    # Evaluate the model\n",
    "    y_pred = combined_model.predict([X_images_test, X_tabular_test])\n",
    "\n",
    "    return combined_model\n",
    "\n",
    "def preprocess_input(user_input):\n",
    "    processed_input = user_input.resize((desired_width, desired_height))  # Resize if needed\n",
    "    processed_input = np.array(processed_input) / 255.0  # Normalize to [0, 1]\n",
    "    processed_input = np.expand_dims(processed_input, axis=0)  # Add batch dimension\n",
    "    return processed_input\n",
    "\n",
    "def make_prediction(model, processed_input):\n",
    "    image_input, tabular_input = processed_input\n",
    "\n",
    "    prediction = model.predict([image_input, tabular_input])\n",
    "\n",
    "    return prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e552e73b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "age_predicton = load_model(load_and_preprocess_data())\n",
    "age_predicton.save('saved_model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e057ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.py\n",
    "\n",
    "from keras.models import load_model\n",
    "\n",
    "def load_age_model():\n",
    "    \n",
    "    # Load the trained model\n",
    "    model = load_model('saved_model.h5')  # Adjust the filename based on your saved model\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa506d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# streamlit_app.py\n",
    "\n",
    "import streamlit as st\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "# Load the model\n",
    "model = load_age_model()\n",
    "\n",
    "def main():\n",
    "    st.title(\"Age Detection App\")\n",
    "\n",
    "    # File uploader for image input\n",
    "    uploaded_file = st.file_uploader(\"Choose an image...\", type=\"jpg\")\n",
    "\n",
    "    if uploaded_file is not None:\n",
    "        # Display the uploaded image\n",
    "        image = Image.open(uploaded_file)\n",
    "        st.image(image, caption=\"Uploaded Image\", use_column_width=True)\n",
    "\n",
    "        # Resize the image to match the model's expected input size\n",
    "        resized_image = image.resize((128, 128))  # Adjust dimensions based on your model\n",
    "        st.image(resized_image, caption=\"Resized Image\", use_column_width=True)\n",
    "\n",
    "        # Preprocess the input\n",
    "        processed_input = preprocess_input(resized_image)\n",
    "\n",
    "        # Make a prediction\n",
    "        prediction = make_prediction(model, processed_input)\n",
    "\n",
    "        # Display the result\n",
    "        # Display the result\n",
    "        st.write(f\"Predicted Age: {model.prediction[0]}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c945f5ed",
   "metadata": {
    "created_in_deepnote_cell": true,
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=0d3193c3-f84c-4987-b2bb-da138af83f70' target=\"_blank\">\n",
    "<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\n",
    "Created in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
